{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingDropout(nn.Module):\n",
    "\n",
    "    \"\"\" Applies dropout in the embedding layer by zeroing out some elements of the embedding vector.\n",
    "    Uses the dropout_mask custom layer to achieve this.\n",
    "    Args:\n",
    "        embed (torch.nn.Embedding): An embedding torch layer\n",
    "        words (torch.nn.Variable): A torch variable\n",
    "        dropout (float): dropout fraction to apply to the embedding weights\n",
    "        scale (float): additional scaling to apply to the modified embedding weights\n",
    "    Returns:\n",
    "        tensor of size: (batch_size x seq_length x embedding_size)\n",
    "    Example:\n",
    "    >> embed = torch.nn.Embedding(10,3)\n",
    "    >> words = Variable(torch.LongTensor([[1,2,4,5] ,[4,3,2,9]]))\n",
    "    >> words.size()\n",
    "        (2,4)\n",
    "    >> embed_dropout_layer = EmbeddingDropout(embed)\n",
    "    >> dropout_out_ = embed_dropout_layer(embed, words, dropout=0.40)\n",
    "    >> dropout_out_\n",
    "        Variable containing:\n",
    "        (0 ,.,.) =\n",
    "          1.2549  1.8230  1.9367\n",
    "          0.0000 -0.0000  0.0000\n",
    "          2.2540 -0.1299  1.5448\n",
    "          0.0000 -0.0000 -0.0000\n",
    "        (1 ,.,.) =\n",
    "          2.2540 -0.1299  1.5448\n",
    "         -4.0457  2.4815 -0.2897\n",
    "          0.0000 -0.0000  0.0000\n",
    "          1.8796 -0.4022  3.8773\n",
    "        [torch.FloatTensor of size 2x4x3]\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embed):\n",
    "        super().__init__()\n",
    "        self.embed = embed\n",
    "\n",
    "    def forward(self, words, dropout=0.1, scale=None):\n",
    "        if dropout:\n",
    "            size = (self.embed.weight.size(0),1)\n",
    "            mask = Variable(dropout_mask(self.embed.weight.data, size, dropout))\n",
    "            masked_embed_weight = mask * self.embed.weight\n",
    "        else: masked_embed_weight = self.embed.weight\n",
    "\n",
    "        if scale: masked_embed_weight = scale * masked_embed_weight\n",
    "\n",
    "        padding_idx = self.embed.padding_idx\n",
    "        if padding_idx is None: padding_idx = -1\n",
    "\n",
    "        X = self.embed._backend.Embedding.apply(words,\n",
    "            masked_embed_weight, padding_idx, self.embed.max_norm,\n",
    "            self.embed.norm_type, self.embed.scale_grad_by_freq, self.embed.sparse)\n",
    "\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def noop(*args, **kwargs): return\n",
    "\n",
    "def repackage_var(h):\n",
    "    \"\"\"Wraps h in new Variables, to detach them from their history.\"\"\"\n",
    "    return Variable(h.data) if type(h) == Variable else tuple(repackage_var(v) for v in h)\n",
    "\n",
    "def dropout_mask(x, sz, dropout):\n",
    "    \"\"\" Applies a dropout mask whose size is determined by passed argument 'sz'.\n",
    "    Args:\n",
    "        x (nn.Variable): A torch Variable object\n",
    "        sz (tuple(int, int, int)): The expected size of the new tensor\n",
    "        dropout (float): The dropout fraction to apply\n",
    "    This method uses the bernoulli distribution to decide which activations to keep.\n",
    "    Additionally, the sampled activations is rescaled is using the factor 1/(1 - dropout).\n",
    "    In the example given below, one can see that approximately .8 fraction of the\n",
    "    returned tensors are zero. Rescaling with the factor 1/(1 - 0.8) returns a tensor\n",
    "    with 5's in the unit places.\n",
    "    The official link to the pytorch bernoulli function is here:\n",
    "        http://pytorch.org/docs/master/torch.html#torch.bernoulli\n",
    "    Examples:\n",
    "        >>> a_Var = torch.autograd.Variable(torch.Tensor(2, 3, 4).uniform_(0, 1), requires_grad=False)\n",
    "        >>> a_Var\n",
    "            Variable containing:\n",
    "            (0 ,.,.) =\n",
    "              0.6890  0.5412  0.4303  0.8918\n",
    "              0.3871  0.7944  0.0791  0.5979\n",
    "              0.4575  0.7036  0.6186  0.7217\n",
    "            (1 ,.,.) =\n",
    "              0.8354  0.1690  0.1734  0.8099\n",
    "              0.6002  0.2602  0.7907  0.4446\n",
    "              0.5877  0.7464  0.4257  0.3386\n",
    "            [torch.FloatTensor of size 2x3x4]\n",
    "        >>> a_mask = dropout_mask(a_Var.data, (1,a_Var.size(1),a_Var.size(2)), dropout=0.8)\n",
    "        >>> a_mask\n",
    "            (0 ,.,.) =\n",
    "              0  5  0  0\n",
    "              0  0  0  5\n",
    "              5  0  5  0\n",
    "            [torch.FloatTensor of size 1x3x4]\n",
    "    \"\"\"\n",
    "    return x.new(*sz).bernoulli_(1-dropout)/(1-dropout)\n",
    "\n",
    "\n",
    "class LockedDropout(nn.Module):\n",
    "    def __init__(self, p=0.5):\n",
    "        super().__init__()\n",
    "        self.p=p\n",
    "\n",
    "    def forward(self, x):\n",
    "        if not self.training or not self.p: return x\n",
    "        m = dropout_mask(x.data, (1, x.size(1), x.size(2)), self.p)\n",
    "        return Variable(m, requires_grad=False) * x\n",
    "\n",
    "\n",
    "class WeightDrop(torch.nn.Module):\n",
    "    \"\"\"A custom torch layer that serves as a wrapper on another torch layer.\n",
    "    Primarily responsible for updating the weights in the wrapped module based\n",
    "    on a specified dropout.\n",
    "    \"\"\"\n",
    "    def __init__(self, module, dropout, weights=['weight_hh_l0']):\n",
    "        \"\"\" Default constructor for the WeightDrop module\n",
    "        Args:\n",
    "            module (torch.nn.Module): A pytorch layer being wrapped\n",
    "            dropout (float): a dropout value to apply\n",
    "            weights (list(str)): the parameters of the wrapped **module**\n",
    "                which should be fractionally dropped.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.module,self.weights,self.dropout = module,weights,dropout\n",
    "        self._setup()\n",
    "\n",
    "    def _setup(self):\n",
    "        \"\"\" for each string defined in self.weights, the corresponding\n",
    "        attribute in the wrapped module is referenced, then deleted, and subsequently\n",
    "        registered as a new parameter with a slightly modified name.\n",
    "        Args:\n",
    "            None\n",
    "         Returns:\n",
    "             None\n",
    "        \"\"\"\n",
    "        if isinstance(self.module, torch.nn.RNNBase): self.module.flatten_parameters = noop\n",
    "        for name_w in self.weights:\n",
    "            w = getattr(self.module, name_w)\n",
    "            del self.module._parameters[name_w]\n",
    "            self.module.register_parameter(name_w + '_raw', nn.Parameter(w.data))\n",
    "\n",
    "\n",
    "    def _setweights(self):\n",
    "        \"\"\" Uses pytorch's built-in dropout function to apply dropout to the parameters of\n",
    "        the wrapped module.\n",
    "        Args:\n",
    "            None\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        for name_w in self.weights:\n",
    "            raw_w = getattr(self.module, name_w + '_raw')\n",
    "            w = torch.nn.functional.dropout(raw_w, p=self.dropout, training=self.training)\n",
    "            if hasattr(self.module, name_w):\n",
    "                delattr(self.module, name_w)\n",
    "            setattr(self.module, name_w, w)\n",
    "\n",
    "    def forward(self, *args):\n",
    "        \"\"\" updates weights and delegates the propagation of the tensor to the wrapped module's\n",
    "        forward method\n",
    "        Args:\n",
    "            *args: supplied arguments\n",
    "        Returns:\n",
    "            tensor obtained by running the forward method on the wrapped module.\n",
    "        \"\"\"\n",
    "        self._setweights()\n",
    "        return self.module.forward(*args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN_Encoder(nn.Module):\n",
    "\n",
    "    \"\"\"A custom RNN encoder network that uses\n",
    "        - an embedding matrix to encode input,\n",
    "        - a stack of LSTM or QRNN layers to drive the network, and\n",
    "        - variational dropouts in the embedding and LSTM/QRNN layers\n",
    "        The architecture for this network was inspired by the work done in\n",
    "        \"Regularizing and Optimizing LSTM Language Models\".\n",
    "        (https://arxiv.org/pdf/1708.02182.pdf)\n",
    "    \"\"\"\n",
    "\n",
    "    initrange=0.1\n",
    "\n",
    "    def __init__(self, ntoken, emb_sz, n_hid, n_layers, pad_token, bidir=False,\n",
    "                 dropouth=0.3, dropouti=0.65, dropoute=0.1, wdrop=0.5, qrnn=False):\n",
    "        \"\"\" Default constructor for the RNN_Encoder class\n",
    "            Args:\n",
    "                bs (int): batch size of input data\n",
    "                ntoken (int): number of vocabulary (or tokens) in the source dataset\n",
    "                emb_sz (int): the embedding size to use to encode each token\n",
    "                n_hid (int): number of hidden activation per LSTM layer\n",
    "                n_layers (int): number of LSTM layers to use in the architecture\n",
    "                pad_token (int): the int value used for padding text.\n",
    "                dropouth (float): dropout to apply to the activations going from one LSTM layer to another\n",
    "                dropouti (float): dropout to apply to the input layer.\n",
    "                dropoute (float): dropout to apply to the embedding layer.\n",
    "                wdrop (float): dropout used for a LSTM's internal (or hidden) recurrent weights.\n",
    "            Returns:\n",
    "                None\n",
    "          \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "        self.ndir                 = 2 if bidir else 1\n",
    "        self.bs, self.qrnn        = 1, qrnn\n",
    "        self.emb_sz               = emb_sz,\n",
    "        self.n_hid                = n_hid\n",
    "        self.n_layers             = n_layers\n",
    "        self.dropoute             = dropoute\n",
    "\n",
    "        \n",
    "        self.encoder              = nn.Embedding(ntoken, emb_sz, padding_idx=pad_token)\n",
    "        self.encoder_with_dropout = EmbeddingDropout(self.encoder)\n",
    "\n",
    "        self.rnns            = [nn.LSTM(emb_sz if l == 0 else n_hid, (n_hid if l != n_layers - 1 else emb_sz)//self.ndir,\n",
    "                                     1, bidirectional=bidir) for l in range(n_layers)]       \n",
    "        if wdrop: self.rnns  = [WeightDrop(rnn, wdrop) for rnn in self.rnns]    \n",
    "        self.rnns = torch.nn.ModuleList(self.rnns)\n",
    "        self.reset_hidden()\n",
    "                                        \n",
    "        self.encoder.weight.data.uniform_(-self.initrange, self.initrange)\n",
    "\n",
    "        self.dropouti  = LockedDropout(dropouti)\n",
    "        self.dropouths = nn.ModuleList([LockedDropout(dropouth) for l in range(n_layers)])\n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\" Invoked during the forward propagation of the RNN_Encoder module.\n",
    "        Args:\n",
    "            input (Tensor): input of shape (sentence length x batch_size)\n",
    "        Returns:\n",
    "            raw_outputs (tuple(list (Tensor), list(Tensor)): list of tensors evaluated from each RNN layer without using\n",
    "            dropouth, list of tensors evaluated from each RNN layer using dropouth,\n",
    "        \"\"\"\n",
    "        print(\"Remi \",type(self.hidden))\n",
    "        sl,bs = input.size()\n",
    "        if bs!=self.bs:\n",
    "            self.bs=bs\n",
    "            self.reset_hidden()\n",
    "            \n",
    "\n",
    "        emb = self.encoder_with_dropout(input, dropout=self.dropoute if self.training else 0)\n",
    "        emb = self.dropouti(emb)\n",
    "        raw_output = emb\n",
    "        new_hidden, raw_outputs, outputs = [],[],[]\n",
    "        for l, (rnn,drop) in enumerate(zip(self.rnns, self.dropouths)):\n",
    "            current_input = raw_output\n",
    "            with warnings.catch_warnings():\n",
    "                warnings.simplefilter(\"ignore\")\n",
    "                raw_output, new_h = rnn(raw_output, self.hidden[l])\n",
    "            new_hidden.append(new_h)\n",
    "            raw_outputs.append(raw_output)\n",
    "            if l != self.n_layers - 1: raw_output = drop(raw_output)\n",
    "            outputs.append(raw_output)\n",
    "\n",
    "            self.hidden = repackage_var(new_hidden)\n",
    "        return raw_outputs, outputs\n",
    "\n",
    "    def one_hidden(self, l):\n",
    "        print(\"Et la:\", l , (self.n_hid if l != self.n_layers - 1 else self.emb_sz[0]))\n",
    "        nh = (self.n_hid if l != self.n_layers - 1 else self.emb_sz[0])//self.ndir\n",
    "        return Variable(self.weights.new(self.ndir, self.bs, nh).zero_(), volatile=not self.training)\n",
    "\n",
    "    def reset_hidden(self):\n",
    "        self.weights = next(self.parameters()).data\n",
    "        self.hidden = [(self.one_hidden(l), self.one_hidden(l)) for l in range(self.n_layers)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearDecoder(nn.Module):\n",
    "    initrange=0.1\n",
    "    def __init__(self, n_out, nhid, dropout, tie_encoder=None, decode_train=True):\n",
    "        super().__init__()\n",
    "        self.decode_train = decode_train\n",
    "        self.decoder = nn.Linear(nhid, n_out, bias=False)\n",
    "        self.decoder.weight.data.uniform_(-self.initrange, self.initrange)\n",
    "        self.dropout = LockedDropout(dropout)\n",
    "        if tie_encoder: self.decoder.weight = tie_encoder.weight\n",
    "\n",
    "    def forward(self, input):\n",
    "        raw_outputs, outputs = input\n",
    "        output = self.dropout(outputs[-1])\n",
    "        output = output.view(output.size(0)*output.size(1), output.size(2))\n",
    "        if self.decode_train or not self.training:\n",
    "            decoded = self.decoder(output)\n",
    "            output = decoded.view(-1, decoded.size(1))\n",
    "        return output, raw_outputs, outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_language_model(n_tok, em_sz, nhid, nlayers, pad_token, decode_train=True, dropouts=None):\n",
    "    if dropouts is None: dropouts = [0.5,0.4,0.5,0.05,0.3]\n",
    "    \n",
    "    rnn_enc = RNN_Encoder(n_tok, em_sz, n_hid=nhid, bidir=False, n_layers=nlayers, pad_token=pad_token,\n",
    "                 dropouti=dropouts[0], wdrop=dropouts[2], dropoute=dropouts[3], dropouth=dropouts[4])\n",
    "    rnn_dec = LinearDecoder(n_tok, em_sz, dropouts[1], decode_train=decode_train, tie_encoder=rnn_enc.encoder)\n",
    "    \n",
    "    return nn.Sequential(rnn_enc, rnn_dec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Et la: 0 1150\n",
      "Et la: 0 1150\n",
      "Et la: 1 1150\n",
      "Et la: 1 1150\n",
      "Et la: 2 400\n",
      "Et la: 2 400\n"
     ]
    }
   ],
   "source": [
    "test_model = get_language_model(n_tok=1000, em_sz=400, nhid=1150, nlayers=3, pad_token=1, decode_train=True, dropouts=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = Variable(torch.LongTensor([[1,2,4,5] ,[4,3,2,9]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remi  [(Variable containing:\n",
      "( 0  ,.,.) = \n",
      "   0   0   0  ...    0   0   0\n",
      "[torch.FloatTensor of size 1x1x1150]\n",
      ", Variable containing:\n",
      "( 0  ,.,.) = \n",
      "   0   0   0  ...    0   0   0\n",
      "[torch.FloatTensor of size 1x1x1150]\n",
      "), (Variable containing:\n",
      "( 0  ,.,.) = \n",
      "   0   0   0  ...    0   0   0\n",
      "[torch.FloatTensor of size 1x1x1150]\n",
      ", Variable containing:\n",
      "( 0  ,.,.) = \n",
      "   0   0   0  ...    0   0   0\n",
      "[torch.FloatTensor of size 1x1x1150]\n",
      "), (Variable containing:\n",
      "( 0 ,.,.) = \n",
      "\n",
      "Columns 0 to 18 \n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "\n",
      "Columns 19 to 37 \n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "\n",
      "Columns 38 to 56 \n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "\n",
      "Columns 57 to 75 \n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "\n",
      "Columns 76 to 94 \n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "\n",
      "Columns 95 to 113 \n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "\n",
      "Columns 114 to 132 \n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "\n",
      "Columns 133 to 151 \n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "\n",
      "Columns 152 to 170 \n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "\n",
      "Columns 171 to 189 \n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "\n",
      "Columns 190 to 208 \n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "\n",
      "Columns 209 to 227 \n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "\n",
      "Columns 228 to 246 \n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "\n",
      "Columns 247 to 265 \n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "\n",
      "Columns 266 to 284 \n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "\n",
      "Columns 285 to 303 \n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "\n",
      "Columns 304 to 322 \n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "\n",
      "Columns 323 to 341 \n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "\n",
      "Columns 342 to 360 \n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "\n",
      "Columns 361 to 379 \n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "\n",
      "Columns 380 to 398 \n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "\n",
      "Columns 399 to 399 \n",
      "    0\n",
      "[torch.FloatTensor of size 1x1x400]\n",
      ", Variable containing:\n",
      "( 0 ,.,.) = \n",
      "\n",
      "Columns 0 to 18 \n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "\n",
      "Columns 19 to 37 \n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "\n",
      "Columns 38 to 56 \n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "\n",
      "Columns 57 to 75 \n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "\n",
      "Columns 76 to 94 \n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "\n",
      "Columns 95 to 113 \n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "\n",
      "Columns 114 to 132 \n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "\n",
      "Columns 133 to 151 \n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "\n",
      "Columns 152 to 170 \n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "\n",
      "Columns 171 to 189 \n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "\n",
      "Columns 190 to 208 \n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "\n",
      "Columns 209 to 227 \n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "\n",
      "Columns 228 to 246 \n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "\n",
      "Columns 247 to 265 \n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "\n",
      "Columns 266 to 284 \n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "\n",
      "Columns 285 to 303 \n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "\n",
      "Columns 304 to 322 \n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "\n",
      "Columns 323 to 341 \n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "\n",
      "Columns 342 to 360 \n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "\n",
      "Columns 361 to 379 \n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "\n",
      "Columns 380 to 398 \n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "\n",
      "Columns 399 to 399 \n",
      "    0\n",
      "[torch.FloatTensor of size 1x1x400]\n",
      ")]\n",
      "Et la: 0 1150\n",
      "Et la: 0 1150\n",
      "Et la: 1 1150\n",
      "Et la: 1 1150\n",
      "Et la: 2 400\n",
      "Et la: 2 400\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-133-354797d03762>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtest_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Miniconda3\\envs\\fastai\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    355\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    356\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 357\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    358\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    359\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\fastai\\lib\\site-packages\\torch\\nn\\modules\\container.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m             \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\fastai\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    355\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    356\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 357\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    358\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    359\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-128-e3a04ef5d8e4>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     76\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcatch_warnings\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m                 \u001b[0mwarnings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msimplefilter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"ignore\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 78\u001b[1;33m                 \u001b[0mraw_output\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnew_h\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrnn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraw_output\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhidden\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     79\u001b[0m             \u001b[0mnew_hidden\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_h\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m             \u001b[0mraw_outputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraw_output\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: tuple index out of range"
     ]
    }
   ],
   "source": [
    "test_model(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3.1.post2\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
