{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib inline\n",
    "# %reload_ext autoreload\n",
    "# %autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image style transfer with Deep learning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will play with a pretrain VGG convolutional neural network in order to understand style transfer <br>\n",
    "At the end, we should be able to take the style of an image A and to apply in to an image B\n",
    "<img src=\"https://dmtyylqvwgyxw.cloudfront.net/instances/132/uploads/images/custom_image/image/1581/normal_Slide11.JPG?v=1508001718\" alt=\"Drawing\" style=\"width: 600px;\"/>\n",
    "<br>\n",
    "This work is based on <a href=\"https://arxiv.org/pdf/1508.06576.pdf\"> this paper</a>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We import our libraries\n",
    "from fastai.conv_learner import *\n",
    "import torch\n",
    "from pathlib import Path\n",
    "from scipy import ndimage\n",
    "torch.cuda.set_device(0)\n",
    "from torchvision import models\n",
    "torch.backends.cudnn.benchmark=True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will download a pretrained m_vgg network. The following cell may take some time to run as you will load the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_vgg = (vgg16(True)).cuda().eval()\n",
    "set_trainable(m_vgg, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part one: The proof that we can reconstruct an image from his convolution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VGG is a convolution neural network created in 2014. It is made of blocks composed of 3 times 3 kernels CNN, with a BatchNord, a Relu and a maxpool. To have a look to its achitecture, run the cell bellow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_vgg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we will prove first is that it is possible to recontruct an image from its convolution output, using backpropagation. Let's take an image, a tree im my case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_fn = \"/home/remi/Desktop/tree.jpg\"\n",
    "img = open_image(img_fn)\n",
    "plt.imshow(img);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We preprocess our image so that we can fit it to the network: <br>\n",
    "(I am running this notebook on my laptop, so I use a small image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sz=288\n",
    "trn_tfms,val_tfms = tfms_from_model(vgg16, sz)\n",
    "img_tfm = val_tfms(img)\n",
    "img_tfm.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we will generate a noise image in wich we will apply our gradient from our convolution activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_img = np.random.uniform(0, 1, size=img.shape).astype(np.float32)\n",
    "plt.imshow(opt_img);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we smooth our image\n",
    "opt_img = scipy.ndimage.filters.median_filter(opt_img, [8,8,1])\n",
    "plt.imshow(opt_img);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_img = val_tfms(opt_img)/2\n",
    "opt_img_v = V(opt_img[None], requires_grad=True)\n",
    "opt_img_v.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We take the first layers of our network, until the first convolution\n",
    "m_vgg = nn.Sequential(*children(m_vgg)[:8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targ_t = m_vgg2(VV(img_tfm[None]))\n",
    "targ_v = V(targ_t)\n",
    "targ_t.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So what we will do is: Take the convolution output of vgg with our tree image, the convolution output of vgg with our noise image, and then retroprapagate the gradient to train the noise image. Here, our loss function will be a basic mean square distance betweet the two outputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_iter = 250\n",
    "show_iter = 5\n",
    "optimizer = optim.LBFGS([opt_img_v], lr=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def actn_loss(x): return F.mse_loss(m_vgg2(x), targ_v)*1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step(loss_fn):\n",
    "    global n_iter\n",
    "    optimizer.zero_grad()\n",
    "    loss = loss_fn(opt_img_v)\n",
    "    loss.backward()\n",
    "    \n",
    "    n_iter+=1\n",
    "    if n_iter%show_iter==0: print(f'Iteration: {n_iter}, loss: {loss.data[0]}')\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iter=0\n",
    "while n_iter <= max_iter: optimizer.step(partial(step,actn_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = val_tfms.denorm(np.rollaxis(to_np(opt_img_v.data),1,4))[0]\n",
    "plt.figure(figsize=(7,7))\n",
    "plt.imshow(x);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import scipy.misc\n",
    "# scipy.misc.imsave('/home/remi/Desktop/content.jpg', x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# extract the style of an image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will do the same but intread of extracting the style, we will extract the content of the image. Do do that, we will compute the Gram matrix from our convolution activation and then we will compare in to the gram matrice of our noise, trying to reduce the distance using backpropagation on the noise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### forward hook\n",
    "We will capture our activation for the input image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SaveFeatures():\n",
    "    features=None\n",
    "    def __init__(self, m): self.hook = m.register_forward_hook(self.hook_fn)\n",
    "    def hook_fn(self, module, input, output): self.features = output\n",
    "    def close(self): self.hook.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_vgg = to_gpu(vgg16(True)).eval()\n",
    "set_trainable(m_vgg, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#List the block we are interested in (=conv output)\n",
    "block_ends = [i-1 for i,o in enumerate(children(m_vgg))\n",
    "              if isinstance(o,nn.MaxPool2d)]\n",
    "block_ends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sf = SaveFeatures(children(m_vgg)[block_ends[3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_opt():\n",
    "    opt_img = np.random.uniform(0, 1, size=img.shape).astype(np.float32)\n",
    "    opt_img = scipy.ndimage.filters.median_filter(opt_img, [8,8,1])\n",
    "    opt_img_v = V(val_tfms(opt_img/2)[None], requires_grad=True)\n",
    "    return opt_img_v, optim.LBFGS([opt_img_v])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_img_v, optimizer = get_opt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_vgg(VV(img_tfm[None]))\n",
    "targ_v = V(sf.features.clone())\n",
    "targ_v.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will extract the style of this painting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# img_fn = \"/home/remi/Desktop/lion.jpg\"\n",
    "# lion = open_image(img_fn)\n",
    "# plt.imshow(lion);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sz=288\n",
    "trn_tfms,val_tfms = tfms_from_model(vgg16, sz)\n",
    "img_tfm = val_tfms(img)\n",
    "img_tfm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def actn_loss2(x):\n",
    "    m_vgg(x)\n",
    "    out = V(sf.features)\n",
    "    return F.mse_loss(out, targ_v)*1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_iter=0\n",
    "# while n_iter <= max_iter: optimizer.step(partial(step,actn_loss2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = val_tfms.denorm(np.rollaxis(to_np(opt_img_v.data),1,4))[0]\n",
    "plt.figure(figsize=(7,7))\n",
    "plt.imshow(x);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sf.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Style match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wget https://raw.githubusercontent.com/jeffxtang/fast-style-transfer/master/images/starry_night.jpg\n",
    "# style_fn = img_fn = \"/home/remi/Desktop/lion.jpg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# style_img = open_image(style_fn)\n",
    "# style_img.shape, img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.imshow(style_img);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = \"/home/remi/Desktop/lion.jpg\"\n",
    "img = open_image(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_match(src, targ):\n",
    "    h,w,_ = src.shape\n",
    "    sh,sw,_ = targ.shape\n",
    "    rat = max(h/sh,w/sw); rat\n",
    "    res = cv2.resize(targ, (int(sw*rat), int(sh*rat)))\n",
    "    return res[:h,:w]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "style = scale_match(img, style_img)\n",
    "style.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(style)\n",
    "style.shape, img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_img_v, optimizer = get_opt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sfs = [SaveFeatures(children(m_vgg)[idx]) for idx in block_ends]\n",
    "m_vgg(VV(img_tfm[None]))\n",
    "targ_vs = [V(o.features.clone()) for o in sfs]\n",
    "[o.shape for o in targ_vs]\n",
    "style_tfm = val_tfms(style_img)\n",
    "m_vgg(VV(style_tfm[None]))\n",
    "targ_styles = [V(o.features.clone()) for o in sfs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[o.shape for o in targ_styles]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gram(input):\n",
    "        b,c,h,w = input.size()\n",
    "        x = input.view(b*c, -1)\n",
    "        return torch.mm(x, x.t())/input.numel()*1e6\n",
    "\n",
    "def gram_mse_loss(input, target): return F.mse_loss(gram(input), gram(target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def style_loss(x):\n",
    "    m_vgg(opt_img_v)\n",
    "    outs = [V(o.features) for o in sfs]\n",
    "    losses = [gram_mse_loss(o, s) for o,s in zip(outs, targ_styles)]\n",
    "    return sum(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "n_iter=0\n",
    "while n_iter <= max_iter: optimizer.step(partial(step,style_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = val_tfms.denorm(np.rollaxis(to_np(opt_img_v.data),1,4))[0]\n",
    "plt.figure(figsize=(7,7))\n",
    "plt.imshow(x);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scipy.misc.imsave('/home/remi/Desktop/style1.jpg', x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.savefig('/home/remi/Documents/code/Jupiter/fastai_old/courses/dl2/data/imagenet/data_style/style2.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sf in sfs: sf.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Reproducing the content on one image with the style of a second image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On this part, we will assemble our two last section. We will take the style of the lion image, the content of the tree image in order to produce a new image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from fastai.conv_learner import *\n",
    "# from pathlib import Path\n",
    "# from scipy import ndimage\n",
    "# torch.cuda.set_device(0)\n",
    "# from torchvision import models\n",
    "# torch.backends.cudnn.benchmark=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_vgg = to_gpu(vgg16(True)).eval()\n",
    "set_trainable(m_vgg, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_fn = \"/home/remi/Desktop/tree.jpg\"\n",
    "img = open_image(img_fn)\n",
    "plt.imshow(img);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "style_fn = \"/home/remi/Desktop/lion.jpg\"\n",
    "style_img = open_image(style_fn)\n",
    "plt.imshow(style_img);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sz=288\n",
    "trn_tfms,val_tfms = tfms_from_model(vgg16, sz)\n",
    "img_tfm = val_tfms(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SaveFeatures():\n",
    "    features=None\n",
    "    def __init__(self, m): self.hook = m.register_forward_hook(self.hook_fn)\n",
    "    def hook_fn(self, module, input, output): self.features = output\n",
    "    def close(self): self.hook.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_opt():\n",
    "    opt_img = np.random.uniform(0, 1, size=img.shape).astype(np.float32)\n",
    "    opt_img = scipy.ndimage.filters.median_filter(opt_img, [8,8,1])\n",
    "    opt_img_v = V(val_tfms(opt_img/2)[None], requires_grad=True)\n",
    "    return opt_img_v, optim.LBFGS([opt_img_v])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step(loss_fn):\n",
    "    global n_iter\n",
    "    optimizer.zero_grad()\n",
    "    loss = loss_fn(opt_img_v)\n",
    "    loss.backward()\n",
    "    n_iter+=1\n",
    "    if n_iter%show_iter==0: print(f'Iteration: {n_iter}, loss: {loss.data[0]}')\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gram(input):\n",
    "        b,c,h,w = input.size()\n",
    "        x = input.view(b*c, -1)\n",
    "        return torch.mm(x, x.t())/input.numel()*1e6\n",
    "\n",
    "def gram_mse_loss(input, target): return F.mse_loss(gram(input), gram(target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sz=288\n",
    "trn_tfms,val_tfms = tfms_from_model(vgg16, sz)\n",
    "\n",
    "opt_img_v, optimizer = get_opt()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set new random image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "block_ends = [i-1 for i,o in enumerate(children(m_vgg))\n",
    "              if isinstance(o,nn.MaxPool2d)]\n",
    "block_ends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#style_tfm = val_tfms(style_img)\n",
    "#m_vgg(VV(style_tfm[None]))\n",
    "#targ_styles = [V(o.features.clone()) for o in sfs]\n",
    "\n",
    "\n",
    "sfs = [SaveFeatures(children(m_vgg)[idx]) for idx in block_ends]\n",
    "m_vgg(VV(img_tfm[None]))\n",
    "targ_vs = [V(o.features.clone()) for o in sfs]\n",
    "[o.shape for o in targ_vs]\n",
    "style_tfm = val_tfms(style_img)\n",
    "m_vgg(VV(style_tfm[None]))\n",
    "targ_styles = [V(o.features.clone()) for o in sfs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targ_styles = [V(o.features.clone()) for o in sfs]\n",
    "\n",
    "def comb_loss(x):\n",
    "    \n",
    "    coef = Variable(torch.from_numpy(np.array([0.2])), requires_grad=False).float()\n",
    "    m_vgg(opt_img_v)\n",
    "    outs = [V(o.features) for o in sfs]\n",
    "    #* coef.expand_as(gram_mse_loss(o, s))\n",
    "    losses = [gram_mse_loss(o, s) for o,s in zip(outs, targ_styles)]\n",
    "    cnt_loss   = F.mse_loss(outs[0], targ_vs[0])*100000*2\n",
    "    style_loss = sum(losses)\n",
    "    return cnt_loss + style_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n_iter=0\n",
    "max_iter=250\n",
    "show_iter=10\n",
    "while n_iter <= max_iter: optimizer.step(partial(step,comb_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = val_tfms.denorm(np.rollaxis(to_np(opt_img_v.data),1,4))[0]\n",
    "plt.figure(figsize=(9,9))\n",
    "plt.imshow(x, interpolation='lanczos')\n",
    "plt.axis('off');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sf in sfs: sf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scipy.misc.imsave('/home/remi/Desktop/transfered.jpg', x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "67px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
